{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64adfdbe",
   "metadata": {},
   "source": [
    "Prompt分类（Prompt-based Classification）是一种新兴的文本分类技术，它通过将任务特定的提示文本（Prompt Text）与输入文本（Input Text）一起输入到预训练语言模型（Pre-trained Language Model）中来实现文本分类。Prompt分类具有高度灵活性和可扩展性，并已经在多个NLP任务中取得了优异的性能。\n",
    "\n",
    "Prompt分类的基本思想是将文本分类任务转化为掩码语言模型（Masked Language Modeling，MLM）任务，通过预测掩码位置（[MASK]）的输出来判断类别。例如，通过文本描述判定天气好坏，类别【好、坏】：常规方法是在BERT模型之后添加一个分类层，哪个输出节点概率最大则划分到哪一类别；而Prompt分类方法是在输入文本前后添加提示文本，并在类别位置添加掩码标记：\n",
    "\n",
    "- 输入：[CLS] 文字描述：今天阳光明媚，微风拂面。 天气：[MASK] [SEP]\n",
    "- 输出：天气：好\n",
    "Prompt分类的优势是可以利用预训练语言模型的强大表达能力和泛化能力，无需额外增加参数或进行微调。Prompt分类的挑战是如何设计合适的提示文本来引导模型进行正确的推理和预测。\n",
    "\n",
    "- 步骤1：加载BERT模型 或 T5模型\n",
    "- 步骤2：将样本加入自定义prompt\n",
    "- 步骤3：使用[MASK]分类进行训练和预测\n",
    "- 步骤4：通过上述步骤，请回答下面问题\n",
    "    - Prompt分类比BERT分类相比，在精度上有什么区别？\n",
    "    - 自定义prompt对模型的精度是否有影响？可以尝试2种不同的prompt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f312ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5Config\n",
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2e3d8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<extra_id_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43madditional_sepcial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m config \u001b[38;5;241m=\u001b[39m T5Config\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model)\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32mD:\\Python\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1790\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 1796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1800\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1801\u001b[0m     )\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "pretrained_model = \"IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese\"\n",
    "\n",
    "special_tokens = [\"<extra_id_{}>\".format(i) for i in range(100)]\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model,\n",
    "                                       do_lower_case=True,\n",
    "                                       max_length=512,\n",
    "                                       truncation=True,\n",
    "                                       additional_sepcial_tokens=special_tokens)\n",
    "\n",
    "config = T5Config.from_pretrained(pretrained_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model, config=config)\n",
    "model.reset_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device=ce)\n",
    "\n",
    "# tokenize\n",
    "text = \"意图识别任务：还有双鸭山到淮阴的汽车票吗13号的 这篇文章的类别是什么？Travel-Query/Music-Play/FilmTele-Play/Video-Play/Radio-Listen/HomeAppliance-Control/Weather-Query/Alarm-Update/Calendar-Query/TVProgram-Play/Audio-Play/Other\"\n",
    "encode_dict = tokenizer(text, max_length=512, padding='max_length',truncation=True)\n",
    "\n",
    "inputs = {\n",
    "  \"input_ids\": torch.tensor([encode_dict['input_ids']]).long().to(device),\n",
    "  \"attention_mask\": torch.tensor([encode_dict['attention_mask']]).long().to(device),\n",
    "  }\n",
    "\n",
    "# generate answer\n",
    "logits = model.generate(\n",
    "  input_ids = inputs['input_ids'],\n",
    "  max_length=100, \n",
    "  do_sample= True\n",
    "  # early_stopping=True,\n",
    "  )\n",
    "\n",
    "logits=logits[:,1:]\n",
    "predict_label = [tokenizer.decode(i,skip_special_tokens=True) for i in logits]\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e511b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集，这里是直接联网读取，也可以通过下载文件，再读取\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = 'https://mirror.coggle.club/dataset/coggle-competition/'\n",
    "train_data = pd.read_csv(data_dir + 'intent-classify/train.csv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv(data_dir + 'intent-classify/test.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a94744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>还有双鸭山到淮阴的汽车票吗13号的</td>\n",
       "      <td>Travel-Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>从这里怎么回家</td>\n",
       "      <td>Travel-Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>随便播放一首专辑阁楼里的佛里的歌</td>\n",
       "      <td>Music-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>给看一下墓王之王嘛</td>\n",
       "      <td>FilmTele-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我想看挑战两把s686打突变团竞的游戏视频</td>\n",
       "      <td>Video-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12095</th>\n",
       "      <td>一千六百五十三加三千一百六十五点六五等于几</td>\n",
       "      <td>Calendar-Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12096</th>\n",
       "      <td>稍小点客厅空调风速</td>\n",
       "      <td>HomeAppliance-Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12097</th>\n",
       "      <td>黎耀祥陈豪邓萃雯畲诗曼陈法拉敖嘉年杨怡马浚伟等到场出席</td>\n",
       "      <td>Radio-Listen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12098</th>\n",
       "      <td>百事盖世群星星光演唱会有谁</td>\n",
       "      <td>Video-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12099</th>\n",
       "      <td>下周一视频会议的闹钟帮我开开</td>\n",
       "      <td>Alarm-Update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0                      1\n",
       "0                还有双鸭山到淮阴的汽车票吗13号的           Travel-Query\n",
       "1                          从这里怎么回家           Travel-Query\n",
       "2                 随便播放一首专辑阁楼里的佛里的歌             Music-Play\n",
       "3                        给看一下墓王之王嘛          FilmTele-Play\n",
       "4            我想看挑战两把s686打突变团竞的游戏视频             Video-Play\n",
       "...                            ...                    ...\n",
       "12095        一千六百五十三加三千一百六十五点六五等于几         Calendar-Query\n",
       "12096                    稍小点客厅空调风速  HomeAppliance-Control\n",
       "12097  黎耀祥陈豪邓萃雯畲诗曼陈法拉敖嘉年杨怡马浚伟等到场出席           Radio-Listen\n",
       "12098                百事盖世群星星光演唱会有谁             Video-Play\n",
       "12099               下周一视频会议的闹钟帮我开开           Alarm-Update\n",
       "\n",
       "[12100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3f10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label处理为T5所需格式\n",
    "label_text = '/'.join(train_data[1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5811952a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = \"意图识别任务：【播放周杰伦的歌曲】 这篇文章的类别是什么？Travel-Query/Music-Play/FilmTele-Play/Video-Play/Radio-Listen/HomeAppliance-Control/Weather-Query/Alarm-Update/Calendar-Query/TVProgram-Play/Audio-Play/Other\"\n",
    "encode_dict = tokenizer(text)\n",
    "\n",
    "inputs = {\n",
    "  \"input_ids\": torch.tensor([encode_dict['input_ids']]).long().to(device),\n",
    "  \"attention_mask\": torch.tensor([encode_dict['attention_mask']]).long().to(device),\n",
    "}\n",
    "\n",
    "logits = model.generate(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    # attention_mask = inputs['attention_mask'],\n",
    "    max_length=20, \n",
    "    do_sample= False\n",
    ")\n",
    "\n",
    "logits=logits[:,1:]\n",
    "predict_label = [tokenizer.decode(i,skip_special_tokens=True) for i in logits]\n",
    "predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f17093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245ab049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\houhailun\\AppData\\Local\\Temp\\ipykernel_35688\\95928455.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for train_text in tqdm_notebook(train_data[0].values):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebf0e1df3ae47b7ac87e719f0f50e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_text \u001b[38;5;129;01min\u001b[39;00m tqdm_notebook(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m      4\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m意图识别任务: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 这篇文章的类别是什么? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     encode_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([encode_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([encode_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 生成answer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "pred_label = []\n",
    "\n",
    "for train_text in tqdm_notebook(train_data[0].values):\n",
    "    text = f\"意图识别任务: {train_text} 这篇文章的类别是什么? {label_text}\"\n",
    "    encode_dict = tokenizer(text, max_length=512, padding='max_length', truncation=True)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor([encode_dict['input_ids']]).long().to(device),\n",
    "        \"attention_mask\": torch.tensor([encode_dict['attention_mask']]).long().to(device),\n",
    "    }\n",
    "    \n",
    "    # 生成answer\n",
    "    logits = model.generate(\n",
    "        input_ids = inputs['input_ids'],\n",
    "        max_length=100,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    logits = logits[:, :]\n",
    "    pred_label += [tokenizer.decode(i,skip_special_tokens=True) for i in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'ID': range(1, len(pred_label) + 1),\n",
    "    'Target': pred_label,\n",
    "}).to_csv('nlp_submit.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
