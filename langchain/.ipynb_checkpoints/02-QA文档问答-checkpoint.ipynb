{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c97fbd",
   "metadata": {},
   "source": [
    "# 文档问答（QA over Documents）\n",
    "为了保证LLM能够执行QA任务\n",
    "1. 需要想LLM传递能够让他参考的上下文信息\n",
    "2. 需要向LLM准确地传达我们的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "878f0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# 继承自 langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32eda763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZhipuAILLM()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ZhipuAILLM()\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6f5bf",
   "metadata": {},
   "source": [
    "## 1. 短文本问答\n",
    "\n",
    "概括来说，使用文档作为上下文进行QA系统的构建过程类似于 llm(context + question) = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb61fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Rachel is 30 years old\n",
    "Bob is 45 years old\n",
    "Kevin is 65 years old\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is under 40 years old?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc42a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rachel is 30 years old\n",
      "Bob is 45 years old\n",
      "Kevin is 65 years old\n",
      "Who is under 40 years old?\n"
     ]
    }
   ],
   "source": [
    "final_prompt = context + question\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3873f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel is under 40 years old.\n"
     ]
    }
   ],
   "source": [
    "output = llm(final_prompt)\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6b75f",
   "metadata": {},
   "source": [
    "## 2. 长文本问答\n",
    "对于长文本，可以对文本分块，对分块的内容进行embedding，将embeding存入向量数据库，然后进行查询\n",
    "\n",
    "目标是选择相关的文本块，但是如何选择呢，选择哪些文本块呢？目前最流行的方法是基于比较向量嵌入来选择相似的文本\n",
    "\n",
    "实现主要步骤\n",
    "实现文档问答系统，可以分为下面5步，每一步langchain都有相关工具。\n",
    "1. 文档加载（Document Loader）：文档加载器把文档加载为langchain能够读取的形式。有不同类型的加载器来加载不同数据源的数据，如CSVLoader、PyPDFLoader、Docx2txtLoader、TextLoader等。\n",
    "2. 文本分割：文本分割器把文档切分为指定大小的分割，分割后的文本称为\"文档块\"\n",
    "3. 向量存储：将上一步中分割好的文档块 以 嵌入 的形式存储到向量数据库中\n",
    "4. 检索Retrival应用程序从存储中检索分割后的文档（例如通过比较余弦相似度，找到与输入问题类似的嵌入片）\n",
    "5. 输出：把问题和相似的嵌入片（文本形式）都放到提示传递给LLM，让LLM生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8fd28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in d:\\python\\envs\\py310\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in d:\\python\\envs\\py310\\lib\\site-packages (from faiss-cpu) (1.25.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu \n",
    "# 需要注意，faiss存在GPU和CPU版本基于你的 runtime 安装对应的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65498265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Embeddings\n",
    "# 分割分文，对分块的内容进行 embedding，将 embedding 存储到数据库中，然后进行查询\n",
    "# 目标是选择相关的文本块，但是我们应该选择哪些文本块呢？目前最流行的方法是基于比较向量嵌入来选择相似的文本\n",
    "\n",
    "from langchain.vectorstores import FAISS  # 向量数据库\n",
    "from langchain.chains import RetrievalQA  # QA检索链\n",
    "from langchain.document_loaders import TextLoader  # 文档加载器\n",
    "\n",
    "# 按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置.在项目中也推荐使用RecursiveCharacterTextSplitter来进行分割。\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings  # 嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7413cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ./data/wonderland.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Python\\envs\\py310\\lib\\site-packages\\langchain_community\\document_loaders\\text.py:42\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 42\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x9d in position 254: illegal multibyte sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/wonderland.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m characters in that document\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Python\\envs\\py310\\lib\\site-packages\\langchain_community\\document_loaders\\text.py:55\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading ./data/wonderland.txt"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('./data/wonderland.txt', 'utf-8')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983faae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
