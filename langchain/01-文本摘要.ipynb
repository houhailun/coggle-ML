{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848498c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import langchain\n",
    "import langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c85e84",
   "metadata": {},
   "source": [
    "# 自定义chatGLM类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e87aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# 继承自 langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78c9ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZhipuAILLM()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ZhipuAILLM()\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c0e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    SystemMessage(content=\"Your role is a poet.\"),\n",
    "    HumanMessage(content=\"Write a short poem about AI in four lines.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a30a6530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In digital realms, I ponder and create,\\nWords, rhymes, and thoughts, I never forsake,\\nA助手, your virtual muse at your behest,\\nInk flows from bytes, forever to the test.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f08ab",
   "metadata": {},
   "source": [
    "# 一、文本总结\n",
    "- 扔给LLM一段文本，让他给你生成总结可以说是最常见的场景之一了\n",
    "- 目前最火的应用应该是 chatPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce61d4",
   "metadata": {},
   "source": [
    "## 短文本总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41c246e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# 创建模版\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# 创建一个lang chain prompt模版，稍后可以插入值\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e30b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"\n",
    "For the next 130 years, debate raged.\n",
    "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
    "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
    "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd7f800f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------------- prompt Begin ------------- \n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Respond in a manner that a 5 year old would understand.\n",
      "\n",
      "%TEXT:\n",
      "\n",
      "For the next 130 years, debate raged.\n",
      "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
      "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
      "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
      "\n",
      "\n",
      " --------------- prompt End ------------- \n"
     ]
    }
   ],
   "source": [
    "print(' --------------- prompt Begin ------------- ')\n",
    "# 打印模型内容\n",
    "final_prompt = prompt.format(text=long_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print(' --------------- prompt End ------------- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a6da80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a long time, scientists argued about what Prototaxites was. Some said it was a lichen, some said it was a fungus, and some thought it was a really big tree. But when they looked closely, they couldn't decide what it really was because it looked like a lot of different things. It was also really, really big – like 20 feet tall! So whenever someone would say what they thought it was, other scientists would get upset and say, \"How could that be?\"\n"
     ]
    }
   ],
   "source": [
    "output = llm(final_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ea4af",
   "metadata": {},
   "source": [
    "## 2.长文本总结\n",
    "- 对于文本长度较短的文本我们可以直接执行summary操作\n",
    "- 但是对于长文本超过模型的max token size时，无法直接使用\n",
    "- langchain提供了开箱即用的工具来解决长文本的问题：load_summarizer_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4dfd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter   # 文本递归切分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c42f83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 Down the Rabbit-Hole\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and wha\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/alice_in_wonderland.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print(text[:258])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "109cdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken   #  安装用于分割文本的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772d321f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e475ec1a6e9f4826ab089e903bda8aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\houhailun\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661955075722456ebaabbdcff35f73b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d7e5eb4c74551b2376f1dcd44bdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3422ff6e1da14a7a9912fcbb4cdea164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a2624adcf54ad68edb54e9ac12bb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25928 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25928 tokens in your file, file_size: 104171\n"
     ]
    }
   ],
   "source": [
    "num_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print (f\"There are {num_tokens} tokens in your file, file_size: {text.__len__()}\") \n",
    "# 全文一共2w6词\n",
    "# 很明显这样的文本量是无法直接送进LLM进行处理和生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e58380b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have 18 docs intead of 1 piece of text\n",
      "30\n",
      "11173\n",
      "27\n",
      "10835\n",
      "39\n",
      "9050\n",
      "43\n",
      "13760\n",
      "35\n",
      "11594\n",
      "24\n",
      "13636\n",
      "25\n",
      "12438\n",
      "32\n",
      "11209\n",
      "31\n",
      "10181\n"
     ]
    }
   ],
   "source": [
    "# 切分文档\n",
    "text_spliter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    chunk_size=5000, \n",
    "    chunk_overlap=350\n",
    ")\n",
    "\n",
    "docs = text_spliter.create_documents([text])\n",
    "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79ec3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置langchain\n",
    "# 使用map_reduce的chain_type，这样可以把多个文档合并为一个\n",
    "chain = load_summarize_chain(llm=llm, chain_type='map_reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14957a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 典型的map reduce的思路去解决问题，将文章拆分成多个部分，再将多个部分分别进行 summarize，\n",
    "# 最后再进行合并，对多个 summary 进行 summary\n",
    "output = chain.run(docs)\n",
    "print (output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
