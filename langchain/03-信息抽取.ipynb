{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cd8a22",
   "metadata": {},
   "source": [
    "# 信息抽取 Extraction\n",
    "- Extraction是从一段文本中解析数据的过程\n",
    "- 通常与Extraction parser一起使用，以构建数据\n",
    "\n",
    "1. 从句子中提取结构化行以插入数据库\n",
    "2. 从长文档中提取多行以插入数据库\n",
    "3. 从用户查询中提取参数以进行API调用\n",
    "4. 最近最火的Extraction库是KOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9829307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# 继承自 langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c28b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ZhipuAILLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8257d0",
   "metadata": {},
   "source": [
    "## 1. 手动格式转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d625f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 解析输出并获取结构化的数据\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2c84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Extraction\n",
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bce0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python dictionary with the fruit names and their corresponding emojis:\n",
      "\n",
      "```python\n",
      "fruit_dict = {\n",
      "    'Apple': '🍎',\n",
      "    'Pear': '🍐',\n",
      "    'Kiwi': '🥝'\n",
      "}\n",
      "```\n",
      "\n",
      "Note: I've included \"Kiwi\" as it was mentioned in your sentence. If you meant \"kiwi\" to be a part of a larger word (like \"this is an kiwi\"), please clarify, and I'll adjust the entry accordingly.\n"
     ]
    }
   ],
   "source": [
    "prompt = instructions + fruit_names\n",
    "\n",
    "# Call the LLM\n",
    "output = llm(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e962e1c",
   "metadata": {},
   "source": [
    "## 自动格式转换\n",
    "自动生成一个带有格式说明的提示\n",
    "\n",
    "这样就不需要担心提示工程的问题了，将这部分完全交给 Lang Chain 来执行\n",
    "\n",
    "将LLM的输出转化为 python 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe090683",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# 解析器将会把LLM的输出使用我定义的schema进行解析并返回期待的结构数据给我\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe15bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a5a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个 Prompt 与之前我们构建 Chat Model 时 Prompt 不同\n",
    "# 这个 Prompt 是一个 ChatPromptTemplate，它会自动将我们的输出转化为 python 对象\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc0a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49cbb087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"artist\": \"Portugal. The Man\",\n",
      "\t\"song\": \"So Young\"\n",
      "}\n",
      "```\n",
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# fruit_output = llm(fruit_query.to_messages())\n",
    "\n",
    "# 自定义的chatglm类与openai在使用上有差别\n",
    "fruit_output = llm.invoke(fruit_query)\n",
    "print(fruit_output)\n",
    "\n",
    "output = output_parser.parse(fruit_output)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
