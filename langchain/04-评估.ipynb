{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1be1cc1",
   "metadata": {},
   "source": [
    "# 结果评估(Evaluation)\n",
    "由于自然语言的不可预测性和可变性，评估LLM的输出是否正确有些困难，langchain 提供了一种方式帮助我们去解决这一难题。\n",
    "\n",
    "- Evaluation是对应用程序的输出进行质量检查的过程\n",
    "- 正常的、确定性的代码有我们可以运行的测试，但由于自然语言的不可预测性和可变性，判断 LLM 的输出更加困难\n",
    "- langchain 提供了一种方式帮助我们去解决这一难题\n",
    "- 对于QApipline 生成的summary进行质量审查\n",
    "- 对 Summary pipline的结果进行检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aabcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# 继承自 langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4594d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ZhipuAILLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a809900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Eval\n",
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee67c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 13637 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# 加载文档\n",
    "loader = TextLoader(\"./data/wonderland.txt\", 'utf-8')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ad2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 6 documents that have an average of 2,272 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "# 文档切分\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_spliter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83a7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化处理\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"D:/code/models/M3E/xrunda/m3e-base\")\n",
    "\n",
    "# 向量化数据库\n",
    "vectorstores = FAISS.from_documents(doc, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db05e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vectorstores.as_retriever(),\n",
    "    input_key=\"question\"\n",
    ")\n",
    "# 注意这里的 input_key 参数，这个参数告诉了 chain 我的问题在字典中的哪个 key 里\n",
    "# 这样 chain 就会自动去找到问题并将其传递给 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "345b4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"Which animal give alice a instruction?\", 'answer' : 'rabbit'},\n",
    "    {'question' : \"What is the author of the book\", 'answer' : 'Elon Mask'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb864141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.apply` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use batch instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which animal give alice a instruction?',\n",
       "  'answer': 'rabbit',\n",
       "  'result': 'The White Rabbit gave Alice instructions.'},\n",
       " {'question': 'What is the author of the book',\n",
       "  'answer': 'Elon Mask',\n",
       "  'result': 'The author of the book you\\'ve described is Lewis Carroll. He wrote \"Alice\\'s Adventures in Wonderland,\" which is the story of Alice and her strange and fantastical experiences in Wonderland. If you have any more questions about the book or its characters, feel free to ask!'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = chain.apply(question_answers)\n",
    "pred\n",
    "# 使用LLM模型进行预测，并将答案与我提供的答案进行比较，这里信任我自己提供的人工答案是正确的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "095ac656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     pred,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7923b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: CORRECT'}, {'results': 'GRADE: INCORRECT'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
