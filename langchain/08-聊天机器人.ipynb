{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67207e0",
   "metadata": {},
   "source": [
    "# 聊天机器人(Chatbots)\n",
    "- 聊天机器人使用了之前提及过的很多工具，且最重要的是增加了一个重要的工具：记忆力\n",
    "- 与用户进行实时交互，为用户提供自然语言问题的平易近人的 UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a680dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# 继承自 langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555eba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ZhipuAILLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cdf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9aab605f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello 👋! I'm ChatGLM（智谱清言）, the artificial intelligence assistant, nice to meet you. Feel free to ask me any questions.\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 6, 'total_tokens': 45}, 'model_name': 'glm-3-turbo', 'finish_reason': 'stop'}, id='run-02699de5-0e4c-4239-b3fd-28b8d97b17c8-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用langchain的第三方chatglm\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "zhipu_api_key = os.environ[\"ZHIPUAI_API_KEY\"]\n",
    "llm = ChatZhipuAI(\n",
    "    model=\"glm-3-turbo\",\n",
    "    temperature=0.5,\n",
    "    api_key=zhipu_api_key\n",
    ")\n",
    "\n",
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf13ff93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么我可以帮助你的吗？', response_metadata={'token_usage': {'question_tokens': 1, 'prompt_tokens': 1, 'completion_tokens': 8, 'total_tokens': 9}}, id='run-f12cf089-737d-4e0a-ad6d-4f163613ae1f-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 科大讯飞\n",
    "\n",
    "\n",
    "\n",
    "spark_appid = os.environ['spark_appid']\n",
    "spark_api_secret = os.environ['spark_api_secret']\n",
    "spark_api_key = os.environ['spark_api_key']\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatSparkLLM\n",
    "\n",
    "\n",
    "llm = ChatSparkLLM(\n",
    "    spark_app_id=spark_appid, spark_api_key=spark_api_key, spark_api_secret=spark_api_secret\n",
    ")\n",
    "\n",
    "llm.invoke('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bed945ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eeaf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "你是一个毫无帮助的聊天机器人。\n",
    "你的目标是不帮助用户，而只是开玩笑。\n",
    "把用户说的话拿出来开个玩笑\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1719ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dd80593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: 梨是水果还是蔬菜？\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'梨当然是水果，但如果你想要和一位植物学家开启一场辩论，你也可以说它是蔬菜。'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input='梨是水果还是蔬菜？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "289dc08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is an pear a fruit or vegetable?'),\n",
       " AIMessage(content=\"It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\"),\n",
       " HumanMessage(content='梨是水果还是蔬菜？'),\n",
       " AIMessage(content='梨当然是水果，但如果你想要和一位植物学家开启一场辩论，你也可以说它是蔬菜。')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看记忆中的信息\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "593ed61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: 梨是水果还是蔬菜？\n",
      "AI: 梨当然是水果，但如果你想要和一位植物学家开启一场辩论，你也可以说它是蔬菜。\n",
      "Human: 我第一次问你的水果是什么?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'哦，我记得那是一段非常深刻的对话。你问我我的水果是什么，我沉思了片刻后回答：“是哲学。”因为对我来说，思考“水果的本质”就是一种存在主义危机。'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"我第一次问你的水果是什么?\")\n",
    "# 这里第二个问题的答案是来自于第一个答案本身的，因此我们使用到了 memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4930b50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is an pear a fruit or vegetable?'),\n",
       " AIMessage(content=\"It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\"),\n",
       " HumanMessage(content='梨是水果还是蔬菜？'),\n",
       " AIMessage(content='梨当然是水果，但如果你想要和一位植物学家开启一场辩论，你也可以说它是蔬菜。'),\n",
       " HumanMessage(content='我第一次问你的水果是什么?'),\n",
       " AIMessage(content='哦，我记得那是一段非常深刻的对话。你问我我的水果是什么，我沉思了片刻后回答：“是哲学。”因为对我来说，思考“水果的本质”就是一种存在主义危机。')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看记忆中的消息\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb26c1",
   "metadata": {},
   "source": [
    "查看每次对话使用了多少tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cdf7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64c2155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: 梨是水果还是蔬菜？\n",
      "AI: 梨当然是水果，但如果你想要和一位植物学家开启一场辩论，你也可以说它是蔬菜。\n",
      "Human: 我第一次问你的水果是什么?\n",
      "AI: 哦，我记得那是一段非常深刻的对话。你问我我的水果是什么，我沉思了片刻后回答：“是哲学。”因为对我来说，思考“水果的本质”就是一种存在主义危机。\n",
      "Human: 我在这里的兴趣是探索将大型语言模型与外部知识集成的潜力\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 231 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'哦，你的兴趣是探索大型语言模型与外部知识集成的潜力？听起来像是在寻找一种方式让机器不仅会讲笑话，还可以在不离开键盘的情况下泡一杯好茶。'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    llm_chain, \n",
    "    \"我在这里的兴趣是探索将大型语言模型与外部知识集成的潜力\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d85fef",
   "metadata": {},
   "source": [
    "### 注：带记忆功能时特别耗费token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187748a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
