{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1c4273-3ba3-47d8-a8bb-51fbcc04b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fc377e-ade1-447c-bba0-764a949120e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model_path = \"../../../models/bert-base-chinese/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35183db-51ec-4cc4-b74b-6c6cd23a1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(text, labels):\n",
    "    # 在 试 选 时 ， 该 县 范 家 梁 村 3 0 0 多 口 人 开 会 选 村 委 会 干 部 ， 大 会 从 晚 上 开 到 第 二 天 天 亮 ， 结 果 选 出 一 个 五 保 户 老 人 ！\n",
    "    # O O O O O O O B-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
    "    # ['在', '试', '选', '时', '，', '该', '县', '范', '家', '梁', '村', '3', '0', '0', '多', '口', '人', '开', '会', '选', '村', '委', '会', '干', '部', '，', '大', '会', '从', '晚', '上', '开', '到', '第', '二', '天', '天', '亮', '，', '结', '果', '选', '出', '一', '个', '五', '保', '户', '老', '人', '！']\n",
    "    # ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'L', 'C', 'I', 'L', 'C', 'I', 'L', 'C', 'I', 'L', 'C', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "    tokenized_text = []\n",
    "    token_labels = []\n",
    "    for word, label in zip(text.split(), labels.split()):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    " \n",
    "        tokenized_text.extend(tokenized_word)\n",
    "        token_labels.extend([label] * n_subwords)\n",
    " \n",
    "    return tokenized_text, token_labels\n",
    " \n",
    "def pad_sequences(sequences, max_len, padding_value=0):\n",
    "    padded_sequences = torch.zeros((len(sequences), max_len)).long()\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        if seq_len <= max_len:\n",
    "            padded_sequences[i, :seq_len] = torch.tensor(seq)\n",
    "        else:\n",
    "            padded_sequences[i, :] = torch.tensor(seq[:max_len])\n",
    "    return padded_sequences\n",
    " \n",
    "def train(model, optimizer, train_dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    " \n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        total_loss += loss.item()\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    return avg_train_loss\n",
    " \n",
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    " \n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            total_loss += loss.item()\n",
    " \n",
    "    avg_eval_loss = total_loss / len(eval_dataloader)\n",
    "    return avg_eval_loss\n",
    " \n",
    "def predict(model, text):\n",
    "    model.eval()\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text_with_labels = [(token, 'O') for token in tokenized_text]\n",
    "    input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        tags = model(input_ids.to(device), attention_mask.to(device))\n",
    " \n",
    "    tag_labels = [id2label[tag] for tag in tags[0]]\n",
    "    return list(zip(tokenized_text, tag_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa78d73-c536-4ac6-998c-f4460af1952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "def load_data():\n",
    "    file_train = '../dataset/msra_ner/train/part.txt'\n",
    "    df = pd.read_csv(file_train, sep='\\t', nrows=1000, header=None)\n",
    "    df.columns = ['text', 'labels']\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "    df['labels'] = df['labels'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66415f9-9445-43ba-8ae2-0546a20b75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC', 'B-LOC', 'I-ORG', 'B-PER', 'O', 'I-PER', 'B-ORG'}\n",
      "--------------------\n",
      "{'I-LOC': 0, 'B-LOC': 1, 'I-ORG': 2, 'B-PER': 3, 'O': 4, 'I-PER': 5, 'B-ORG': 6}\n",
      "--------------------\n",
      "{0: 'I-LOC', 1: 'B-LOC', 2: 'I-ORG', 3: 'B-PER', 4: 'O', 5: 'I-PER', 6: 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "def make_label_id_dict(df):\n",
    "    labels = [x.split() for x in df.labels.values.tolist()]\n",
    "    # 标签去重\n",
    "    unique_labels = set()\n",
    "    for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb]\n",
    "        \n",
    "    print(unique_labels)\n",
    "        \n",
    "    label2id = {v: k for k, v in enumerate(unique_labels)}\n",
    "    id2label = {k: v for k, v in enumerate(unique_labels)}\n",
    "    \n",
    "    return label2id, id2label, unique_labels, labels\n",
    "\n",
    "label2id, id2label, unique_labels, all_labels = make_label_id_dict(df)\n",
    "\n",
    "print('-' * 20)\n",
    "print(label2id)\n",
    "print('-' * 20)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09887ec-01d0-41a8-bffc-20050f68a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 切分训练集、验证集、测试集\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "train_data = df_train.values.tolist()\n",
    "test_data = df_test.values.tolist()\n",
    "val_data = df_val.values.tolist()\n",
    "\n",
    " \n",
    "# 同样地，我们还需要加载验证集和测试集，并将它们转换为模型所需的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79a48def-5af8-4e04-8a21-6bbadc99b085",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m makeDataLoader(test_data)\n\u001b[0;32m     36\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m makeDataLoader(val_data)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def makeDataLoader(data):\n",
    "    # 将数据集转换为模型所需的格式\n",
    "    train_input_ids = []\n",
    "    train_attention_masks = []\n",
    "    train_labels = []\n",
    "    \n",
    "    # 对每一个word做label补齐\n",
    "    for words, label in data:\n",
    "        try:\n",
    "            tokenized_text, token_labels = tokenize_and_preserve_labels(words, label)\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "         \n",
    "            train_input_ids.append(input_ids)\n",
    "            train_attention_masks.append(attention_mask)\n",
    "            train_labels.append([label2id[label] for label in token_labels])\n",
    "        except:\n",
    "            print(words)\n",
    "            print(label)\n",
    "            print(tokenized_text)\n",
    "            print(token_labels)\n",
    "            aaa\n",
    "     \n",
    "    train_input_ids = pad_sequences(train_input_ids, MAX_LEN)\n",
    "    train_attention_masks = pad_sequences(train_attention_masks, MAX_LEN)\n",
    "    train_labels = pad_sequences(train_labels, MAX_LEN, padding_value=-1)\n",
    "     \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "train_dataloader = makeDataLoader(train_data)\n",
    "test_dataloader = makeDataLoader(test_data)\n",
    "val_dataloader = makeDataLoader(val_data)\n",
    "\n",
    "print(train_dataloader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f406023e-8cf4-4c3c-b449-265d78218058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size, num_tags):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bilstm = nn.LSTM(bidirectional=True, input_size=hidden_size, hidden_size=hidden_size // 2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags)\n",
    " \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        lstm_output, _ = self.bilstm(sequence_output)\n",
    "        logits = self.fc(lstm_output)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            tags = self.crf.decode(logits, mask=attention_mask.byte())\n",
    "            return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "622d4d6c-04f5-4650-9dcd-092c2fae384d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 11\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     avg_eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_dataloader)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_eval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_dataloader)\u001b[0m\n\u001b[0;32m     30\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m---> 32\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import torch.optim as optim\n",
    "\n",
    "# 训练模型\n",
    "model = EntityModel(bert_model, hidden_size=768, num_tags=len(label2id))\n",
    "model.to(device)\n",
    " \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss = train(model, optimizer, train_dataloader)\n",
    "    avg_eval_loss = evaluate(model, test_dataloader)\n",
    "    print(f'Epoch {epoch + 1}: train_loss={avg_train_loss:.4f}, eval_loss={avg_eval_loss:.4f}')\n",
    " \n",
    "# 测试模型\n",
    "test_sentences = ['今天是个好日子', '我喜欢中国菜', '巴黎是一座美丽的城市']\n",
    "for sentence in test_sentences:\n",
    "    tags = predict(model, sentence)\n",
    "    print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d762f-23e3-4ef9-9ab3-416d3df7fea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
