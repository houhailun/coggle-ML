{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1598fe-b0a3-456e-b5ef-677d2e404806",
   "metadata": {},
   "source": [
    "# 使用BERT+LSTM+CRF 实现NER任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06454512-da0a-48ba-95c9-bd75ea7e52c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf in d:\\python\\envs\\py310\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: seqeval in d:\\python\\envs\\py310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in d:\\python\\envs\\py310\\lib\\site-packages (from seqeval) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\python\\envs\\py310\\lib\\site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\python\\envs\\py310\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\python\\envs\\py310\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\envs\\py310\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-crf seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fb5174-55b6-4d53-83fa-da174a6970a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4dbc2e-76c2-4821-8c3e-35775e9a3b84",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06835bf-10de-40d9-81af-f165080d1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    file_train = '../dataset/msra_ner/train/part.txt'\n",
    "    df = pd.read_csv(file_train, sep='\\t', nrows=1000, header=None)\n",
    "    df.columns = ['text', 'labels']\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "    df['labels'] = df['labels'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60530132-5181-4390-8234-a451739dfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe58bb5-f572-4e62-8233-a4141c935842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-PER': 0, 'O': 1, 'B-ORG': 2, 'I-PER': 3, 'I-LOC': 4, 'B-LOC': 5, 'I-ORG': 6}\n",
      "--------------------\n",
      "{0: 'B-PER', 1: 'O', 2: 'B-ORG', 3: 'I-PER', 4: 'I-LOC', 5: 'B-LOC', 6: 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_label_id_dict(df):\n",
    "    labels = [x.split() for x in df.labels.values.tolist()]\n",
    "    # 标签去重\n",
    "    unique_labels = set()\n",
    "    for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb]\n",
    "        \n",
    "    # print(unique_labels)\n",
    "        \n",
    "    label2id = {v: k for k, v in enumerate(unique_labels)}\n",
    "    id2label = {k: v for k, v in enumerate(unique_labels)}\n",
    "    \n",
    "    return label2id, id2label, unique_labels, labels\n",
    "\n",
    "label2id, id2label, unique_labels, labels = make_label_id_dict(df)\n",
    "\n",
    "print(label2id)\n",
    "print('-' * 20)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e92289-ec04-436e-beb4-d19bf2005305",
   "metadata": {},
   "source": [
    "# 构建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152ce29d-1f0f-4111-9a91-eeda51e85e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 5\n",
    "\n",
    "model_path = \"../../../models/bert-base-chinese/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b846f40-099d-4823-a5fe-ab2b36d79bf7",
   "metadata": {},
   "source": [
    "BERT做NER 一个棘手部分是 BERT 依赖于 wordpiece tokenization，而不是 word tokenization。比如：Washington的标签为 \"b-gpe\",分词之后得到， \"Wash\", \"##ing\", \"##ton\",\"b-gpe\", \"b-gpe\", \"b-gpe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac75281-1b22-4e5e-b916-4a9295675651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"    \n",
    "    Word piece tokenization使得很难将词标签与单个subword进行匹配。\n",
    "    这个函数每次次对每个单词进行一个分词，这样方便为每个subword保留正确的标签。 \n",
    "    当然，它的处理时间有点慢，但它会帮助我们的模型达到更高的精度。\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "\n",
    "        # 逐字分词\n",
    "        tokenized_word = tokenizer.tokenize(word) # id\n",
    "        n_subwords = len(tokenized_word) # 1\n",
    "\n",
    "        # 将单个字分词结果追加到句子分词列表\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # 标签同样添加n个subword，与原始word标签一致\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03288f0-4699-4cc3-907c-34a850f850dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                    海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "labels    O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df.iloc[0]\n",
    "print(example)\n",
    "tokenize_and_preserve_labels(example['text'], example['labels'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ac2bb1-7781-4113-bb58-bcf65322d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 步骤 1: 对每个句子分词\n",
    "        sentence = self.data.text[index]  \n",
    "        word_labels = self.data.labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # 步骤 2: 添加特殊token并添加对应的标签\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # 给[CLS] token添加O标签\n",
    "        labels.insert(-1, \"O\") # 给[SEP] token添加O标签\n",
    "\n",
    "        # 步骤 3: 截断/填充\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # 截断\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # 填充\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # 步骤 4: 构建attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # 步骤 5: 将分词结果转为词表的id表示\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558934c1-0a4d-450f-b5ea-2ad6251e57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1000, 2)\n",
      "TRAIN Dataset: (800, 2)\n",
      "TEST Dataset: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train_dataset,test_dataset=train_test_split(data,test_size=0.2,random_state=42)\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size,random_state=200)\n",
    "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c55f4cb-def5-4e70-95ad-7fe248f951de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([ 101, 1377, 3221, 8024, 2357, 5812, 2209, 4638,  976, 3791, 6901, 1168,\n",
       "          671,  763, 2768, 1447, 1744, 3124, 2424, 7566, 2193, 1469, 3616, 3828,\n",
       "         6379,  833, 6379, 7270, 4638, 2900, 6569, 8024, 2548, 1744, 2600, 4415,\n",
       "         4906, 2209, 1469,  800, 4638, 6568, 7270, 7794, 3419, 2209,  738, 1728,\n",
       "         3634,  679, 4721,  511,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'targets': tensor([1, 1, 1, 1, 0, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6,\n",
       "         6, 6, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 0, 3, 1, 1, 1, 1, 1, 0, 3, 3, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4f85ea-bf8a-4f10-8d22-c8e81248a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed214880-ebbd-4312-a451-679a939ba5f3",
   "metadata": {},
   "source": [
    "# 定义网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528f7e0-8819-4678-8f63-96483854cef2",
   "metadata": {},
   "source": [
    "- 模型结构：BertForTokenClassification\r",
    "- \n",
    "预训练权重： \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8650a1b1-379a-4caa-886e-d4e7c190a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../../../models/bert-base-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(model_path, num_labels=len(label2id))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18764dfe-433f-4497-8a8b-ffd47b06bd92",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1671ab5-54cb-4ee7-b1c9-a427f9b4b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 100]) torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2627, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.Size([1, 91])\n",
    "# unsqueeze(0): 在最前面位置添加1个维度：[100] -> [1, 100]\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0) # 真实标签\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "print(ids.shape, mask.shape, targets.shape)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets) # 输出有两个：一个为loss和一个为logits\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f423f-8220-4ef8-af53-1b7ce0237650",
   "metadata": {},
   "source": [
    "模型输出logits大小为 (batch_size, sequence_length, num_labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edd51e7b-0eb0-4132-ad8e-54801c845a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9995b-a563-4443-996f-8d11ed6dbaa6",
   "metadata": {},
   "source": [
    "设置优化器Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d7b502a-7185-485b-aa1d-8e62d89f1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 训练函数\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # 将model设置为train模式\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long) #(4, 100)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long) #(4, 100)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)#(4, 100)\n",
    "        \n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs[0],outputs[1]\n",
    "        # print(outputs.keys())\n",
    "        # print(loss)\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 50==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 50 training steps: {loss_step}\")\n",
    "           \n",
    "        # 计算准确率\n",
    "        flattened_targets = targets.view(-1) # 真实标签 大小 (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # 模型输出shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # 取出每个token对应概率最大的标签索引 shape (batch_size * seq_len,)\n",
    "        # MASK：PAD\n",
    "        # 只保留非pad部分\n",
    "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # 梯度剪切\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # loss反向求导\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66dc4e60-5c48-4415-ad39-5f4e9be15c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 50 training steps: 2.229024648666382\n",
      "Training loss per 50 training steps: 0.47690444271646293\n",
      "Training loss per 50 training steps: 0.33161160331403855\n",
      "Training loss per 50 training steps: 0.26141892986631154\n",
      "Training loss epoch: 0.21976265305187553\n",
      "Training accuracy epoch: 0.9007162400403288\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcf15c-8342-4c88-9317-1b4f1b375c10",
   "metadata": {},
   "source": [
    "# 评估模型\n",
    "验证集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23eec965-371f-418a-926d-60698c9b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.11738655716180801\n",
      "Validation Loss: 0.06324117779149674\n",
      "Validation Accuracy: 0.9613891533189418\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            # loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs[0],outputs[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # 计算准确率\n",
    "            flattened_targets = targets.view(-1) # 大小 (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size * seq_len,)\n",
    "            \n",
    "            active_accuracy = mask.view(-1) == 1 # 大小 (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions\n",
    "\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1489e341-39f1-40d6-9aa7-be94049df642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        41790\n",
       "I-ORG     1898\n",
       "I-LOC     1065\n",
       "I-PER      857\n",
       "B-LOC      782\n",
       "B-ORG      481\n",
       "B-PER      407\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=[]\n",
    "for tags in df['labels']:\n",
    "    tmp.extend(tags.split())\n",
    "    \n",
    "pd.Series(tmp).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71300613-6598-440a-bc18-acd89663357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG']\n",
      "9373\n",
      "----------------------------------------\n",
      "['O', 'O', 'B-LOC', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "9373\n"
     ]
    }
   ],
   "source": [
    "print(labels[0:20])\n",
    "print(len(labels))\n",
    "print('-' * 40)\n",
    "print(predictions[0:20])\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a539d54a-eebb-4627-b3a6-86f836e3306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.54      0.64      0.59       153\n",
      "         ORG       0.27      0.39      0.32       105\n",
      "         PER       0.80      0.88      0.84        69\n",
      "\n",
      "   micro avg       0.49      0.61      0.54       327\n",
      "   macro avg       0.54      0.64      0.58       327\n",
      "weighted avg       0.51      0.61      0.55       327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbee2e-d7be-4988-aa35-15168f26e240",
   "metadata": {},
   "source": [
    "> 结论分类的准召比较差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec0c3f-3aeb-49f6-97f3-a5a40b4f3e98",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab4340c1-b243-4d38-936e-e11bab4bb132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "侯 海 伦 在 石 家 庄 御 芝 林 公 司 担 任 算 法 工 程 师 。\n",
      "['B-PER', 'I-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"侯海伦在石家庄御芝林公司担任算法工程师。\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# 加载到gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# 输入到模型\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size*seq_len,) \n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# 拼接文本\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c5770-feec-4eb4-b4ea-5b5cf2e1a4fe",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169264c-5b5b-4e01-b953-c946729e05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./model\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# 保存tokenizer\n",
    "tokenizer.save_vocabulary(directory)\n",
    "# 保存权重和配置文件\n",
    "model.save_pretrained(directory)\n",
    "print('All files saved')\n",
    "print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1cc79-116f-48f1-8bad-4386e0d42116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
