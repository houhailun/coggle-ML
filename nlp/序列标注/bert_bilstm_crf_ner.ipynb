{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1c4273-3ba3-47d8-a8bb-51fbcc04b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fc377e-ade1-447c-bba0-764a949120e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model_path = \"../../../models/bert-base-chinese/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35183db-51ec-4cc4-b74b-6c6cd23a1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(text, labels):\n",
    "    tokenized_text = []\n",
    "    token_labels = []\n",
    "    for word, label in zip(text.split(), labels.split()):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    " \n",
    "        tokenized_text.extend(tokenized_word)\n",
    "        token_labels.extend([label] * n_subwords)\n",
    " \n",
    "    return tokenized_text, token_labels\n",
    " \n",
    "def pad_sequences(sequences, max_len, padding_value=0):\n",
    "    padded_sequences = torch.zeros((len(sequences), max_len)).long()\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = len(seq)\n",
    "        if seq_len <= max_len:\n",
    "            padded_sequences[i, :seq_len] = torch.tensor(seq)\n",
    "        else:\n",
    "            padded_sequences[i, :] = torch.tensor(seq[:max_len])\n",
    "    return padded_sequences\n",
    " \n",
    "def train(model, optimizer, train_dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        print(input_ids.shape)\n",
    "        print(attention_mask.shape)\n",
    "        print(labels.shape)\n",
    "        \n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        total_loss += loss.item()\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    return avg_train_loss\n",
    " \n",
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    " \n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            total_loss += loss.item()\n",
    " \n",
    "    avg_eval_loss = total_loss / len(eval_dataloader)\n",
    "    return avg_eval_loss\n",
    " \n",
    "def predict(model, text):\n",
    "    model.eval()\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text_with_labels = [(token, 'O') for token in tokenized_text]\n",
    "    input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        tags = model(input_ids.to(device), attention_mask.to(device))\n",
    " \n",
    "    tag_labels = [id2label[tag] for tag in tags[0]]\n",
    "    return list(zip(tokenized_text, tag_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa78d73-c536-4ac6-998c-f4460af1952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "def load_data():\n",
    "    file_train = '../dataset/msra_ner/train/part.txt'\n",
    "    df = pd.read_csv(file_train, sep='\\t', nrows=1000, header=None)\n",
    "    df.columns = ['text', 'labels']\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "    df['labels'] = df['labels'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66415f9-9445-43ba-8ae2-0546a20b75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC', 'B-ORG', 'I-PER', 'I-ORG', 'B-PER', 'O', 'I-LOC'}\n",
      "--------------------\n",
      "{'B-LOC': 0, 'B-ORG': 1, 'I-PER': 2, 'I-ORG': 3, 'B-PER': 4, 'O': 5, 'I-LOC': 6}\n",
      "--------------------\n",
      "{0: 'B-LOC', 1: 'B-ORG', 2: 'I-PER', 3: 'I-ORG', 4: 'B-PER', 5: 'O', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "def make_label_id_dict(df):\n",
    "    labels = [x.split() for x in df.labels.values.tolist()]\n",
    "    # 标签去重\n",
    "    unique_labels = set()\n",
    "    for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb]\n",
    "        \n",
    "    print(unique_labels)\n",
    "        \n",
    "    label2id = {v: k for k, v in enumerate(unique_labels)}\n",
    "    id2label = {k: v for k, v in enumerate(unique_labels)}\n",
    "    \n",
    "    return label2id, id2label, unique_labels, labels\n",
    "\n",
    "label2id, id2label, unique_labels, all_labels = make_label_id_dict(df)\n",
    "\n",
    "print('-' * 20)\n",
    "print(label2id)\n",
    "print('-' * 20)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09887ec-01d0-41a8-bffc-20050f68a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 切分训练集、验证集、测试集\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "train_data = df_train.values.tolist()\n",
    "test_data = df_test.values.tolist()\n",
    "val_data = df_val.values.tolist()\n",
    "\n",
    " \n",
    "# 同样地，我们还需要加载验证集和测试集，并将它们转换为模型所需的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a48def-5af8-4e04-8a21-6bbadc99b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataLoader(data):\n",
    "    # 将数据集转换为模型所需的格式\n",
    "    train_input_ids = []\n",
    "    train_attention_masks = []\n",
    "    train_labels = []\n",
    "    \n",
    "    # 对每一个word做label补齐\n",
    "    for words, label in data:\n",
    "        try:\n",
    "            tokenized_text, token_labels = tokenize_and_preserve_labels(words, label)\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "         \n",
    "            train_input_ids.append(input_ids)\n",
    "            train_attention_masks.append(attention_mask)\n",
    "            train_labels.append([label2id[label] for label in token_labels])\n",
    "        except:\n",
    "            print(words)\n",
    "            print(label)\n",
    "            print(tokenized_text)\n",
    "            print(token_labels)\n",
    "            aaa\n",
    "     \n",
    "    train_input_ids = pad_sequences(train_input_ids, MAX_LEN)\n",
    "    train_attention_masks = pad_sequences(train_attention_masks, MAX_LEN)\n",
    "    train_labels = pad_sequences(train_labels, MAX_LEN, padding_value=-1)\n",
    "     \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "train_dataloader = makeDataLoader(train_data)\n",
    "test_dataloader = makeDataLoader(test_data)\n",
    "val_dataloader = makeDataLoader(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f406023e-8cf4-4c3c-b449-265d78218058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size, num_tags):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bilstm = nn.LSTM(bidirectional=True, input_size=hidden_size, hidden_size=hidden_size // 2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    " \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        lstm_output, _ = self.bilstm(sequence_output)\n",
    "        logits = self.fc(lstm_output)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            tags = self.crf.decode(logits, mask=attention_mask.byte())\n",
    "            return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622d4d6c-04f5-4650-9dcd-092c2fae384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\torchcrf\\__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "Epoch 1: train_loss=1257.8422, eval_loss=605.3266\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 11\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     avg_eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_dataloader)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_eval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_dataloader)\u001b[0m\n\u001b[0;32m     41\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 44\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[1;32mD:\\Python\\envs\\py310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\envs\\py310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import torch.optim as optim\n",
    "\n",
    "# 训练模型\n",
    "model = EntityModel(bert_model, hidden_size=768, num_tags=len(label2id))\n",
    "model.to(device)\n",
    " \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss = train(model, optimizer, train_dataloader)\n",
    "    avg_eval_loss = evaluate(model, test_dataloader)\n",
    "    print(f'Epoch {epoch + 1}: train_loss={avg_train_loss:.4f}, eval_loss={avg_eval_loss:.4f}')\n",
    " \n",
    "# 测试模型\n",
    "test_sentences = ['今天是个好日子', '我喜欢中国菜', '巴黎是一座美丽的城市']\n",
    "for sentence in test_sentences:\n",
    "    tags = predict(model, sentence)\n",
    "    print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d762f-23e3-4ef9-9ab3-416d3df7fea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
