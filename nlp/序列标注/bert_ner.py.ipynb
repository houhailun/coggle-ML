{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5561ab",
   "metadata": {},
   "source": [
    "# NER 序列标注"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a72ae5",
   "metadata": {},
   "source": [
    "## baseline：bert + softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075e2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939f832",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d47caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。</td>\n",
       "      <td>O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 ...</td>\n",
       "      <td>B-LOC B-LOC O O O O O O O O O O O O O O B-LOC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>克 马 尔 的 女 儿 让 娜 今 年 读 五 年 级 ， 她 所 在 的 班 上 有 3 ...</td>\n",
       "      <td>B-PER I-PER I-PER O O O B-PER I-PER O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>参 加 步 行 的 有 男 有 女 ， 有 年 轻 人 ， 也 有 中 年 人 。</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>沙 特 队 教 练 佩 雷 拉 ： 两 支 队 都 想 胜 ， 因 此 都 作 出 了 最 ...</td>\n",
       "      <td>B-ORG I-ORG I-ORG O O B-PER I-PER I-PER O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>这 种 混 乱 局 面 导 致 有 些 海 域 使 用 者 的 合 法 权 益 难 以 得 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>鲁 宾 明 确 指 出 ， 对 政 府 的 这 种 指 控 完 全 没 有 事 实 根 据 ...</td>\n",
       "      <td>B-PER I-PER O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。   \n",
       "1  这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ...   \n",
       "2  但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 ...   \n",
       "3  在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 ...   \n",
       "4  日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 ...   \n",
       "5  克 马 尔 的 女 儿 让 娜 今 年 读 五 年 级 ， 她 所 在 的 班 上 有 3 ...   \n",
       "6          参 加 步 行 的 有 男 有 女 ， 有 年 轻 人 ， 也 有 中 年 人 。   \n",
       "7  沙 特 队 教 练 佩 雷 拉 ： 两 支 队 都 想 胜 ， 因 此 都 作 出 了 最 ...   \n",
       "8  这 种 混 乱 局 面 导 致 有 些 海 域 使 用 者 的 合 法 权 益 难 以 得 ...   \n",
       "9  鲁 宾 明 确 指 出 ， 对 政 府 的 这 种 指 控 完 全 没 有 事 实 根 据 ...   \n",
       "\n",
       "                                              labels  \n",
       "0  O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O ...  \n",
       "1  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "2  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "3  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "4  B-LOC B-LOC O O O O O O O O O O O O O O B-LOC ...  \n",
       "5  B-PER I-PER I-PER O O O B-PER I-PER O O O O O ...  \n",
       "6          O O O O O O O O O O O O O O O O O O O O O  \n",
       "7  B-ORG I-ORG I-ORG O O B-PER I-PER I-PER O O O ...  \n",
       "8  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "9  B-PER I-PER O O O O O O O O O O O O O O O O O ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    file_train = '../dataset/msra_ner/train/part.txt'\n",
    "    df = pd.read_csv(file_train, sep='\\t', nrows=1000, header=None)\n",
    "    df.columns = ['text', 'labels']\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "    df['labels'] = df['labels'].apply(lambda x: x.replace('\u0002', ' '))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4de2a",
   "metadata": {},
   "source": [
    "### label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7953529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC', 'I-PER', 'I-ORG', 'B-PER', 'O', 'B-ORG', 'B-LOC'}\n",
      "{'I-LOC': 0, 'I-PER': 1, 'I-ORG': 2, 'B-PER': 3, 'O': 4, 'B-ORG': 5, 'B-LOC': 6}\n",
      "--------------------\n",
      "{0: 'I-LOC', 1: 'I-PER', 2: 'I-ORG', 3: 'B-PER', 4: 'O', 5: 'B-ORG', 6: 'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "def make_label_id_dict(df):\n",
    "    labels = [x.split() for x in df.labels.values.tolist()]\n",
    "    # 标签去重\n",
    "    unique_labels = set()\n",
    "    for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb]\n",
    "        \n",
    "    print(unique_labels)\n",
    "        \n",
    "    label2id = {v: k for k, v in enumerate(unique_labels)}\n",
    "    id2label = {k: v for k, v in enumerate(unique_labels)}\n",
    "    \n",
    "    return label2id, id2label, unique_labels, labels\n",
    "\n",
    "label2id, id2label, unique_labels, labels = make_label_id_dict(df)\n",
    "\n",
    "print(label2id)\n",
    "print('-' * 20)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b9a50",
   "metadata": {},
   "source": [
    "### tokenizer化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c5bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../models/bert-base-chinese/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f95987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce5b7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "----------------------------------------\n",
      "{'input_ids': tensor([[ 101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305,  680, 7032,\n",
      "         7305,  722, 7313, 4638, 3862, 1818,  511,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "----------------------------------------\n",
      "[CLS] 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------\n",
      "['[CLS]', '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "----------------------------------------\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "text = df['text'].values.tolist()\n",
    "example = text[0]\n",
    "text_tokenized = tokenizer(example, \n",
    "                           padding='max_length', \n",
    "                           max_length=50, \n",
    "                           truncation=True,\n",
    "                           return_tensors=\"pt\")\n",
    "print(example)\n",
    "print('-' * 40)\n",
    "print(text_tokenized)\n",
    "print('-' * 40)\n",
    "# 解码为原始text\n",
    "print(tokenizer.decode(text_tokenized.input_ids[0]))\n",
    "\n",
    "print('-' * 40)\n",
    "# NOTE:tokenizer后可能有以下问题：\n",
    "    # BERT 分词器在底层使用了所谓的 word-piece tokenizer，它是一个子词分词器。这意味着 BERT tokenizer 可能会将一个词拆分为一个或多个有意义的子词\n",
    "    # 导致tokenizer后的序列长度和原始label长度不一致\n",
    "    # 需要调整标签，以达到一一对应的结果。使其与标记化后的序列具有相同的长度\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n",
    "print('-' * 40)\n",
    "\n",
    "# 使用word_ids来调整\n",
    "# 每个拆分的 token 共享相同的 word_ids，其中来自 BERT 的特殊 token，例如 [CLS]、[SEP] 和 [PAD] 都没有特定word_ids的，结果是None\n",
    "word_ids = text_tokenized.word_ids()\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b03733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "----------------------------------------\n",
      "[-100, 4, 4, 4, 4, 4, 4, 4, 6, 0, 4, 6, 0, 4, 4, 4, 4, 4, 4, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "----------------------------------------\n",
      "['[CLS]', '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 方法1：只为每个拆分token的第一个子词提供一个标签。子词的延续将简单地用\"-100\"作为标签。所有没有word_ids 的token也将标为 \"-100\"。\n",
    "# 方法2：在属于同一 token 的所有子词中提供相同的标签。所有没有word_ids的token都将标为 \"-100\"。\n",
    "# \"\"\"\n",
    "def align_label_example(tokenized_input, labels, label_all_tokens, labels_to_ids):\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    pre_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        # 把特殊token的label置为-100\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # 当前id是新的词或者单词的第一个子词\n",
    "        elif word_idx != pre_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            # 当前id是上一个词的子词\n",
    "            label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "        pre_word_idx = word_idx\n",
    "    return label_ids\n",
    "\n",
    "label = labels[0]\n",
    "print(label)\n",
    "print('-' * 40)\n",
    "new_label = align_label_example(text_tokenized, label, True, label2id)\n",
    "print(new_label)\n",
    "print('-' * 40)\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51688607",
   "metadata": {},
   "source": [
    "### dataset构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a08cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "# 创建一个Dataset类来批量生成和获取数据\n",
    "\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    # 首先tokenizer输入文本\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "    # 获取word_ids\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    # 采用上述的第2种方法来调整标签，使得标签与输入数据对其。\n",
    "    for word_idx in word_ids:\n",
    "        # 如果token不在word_ids内，则用 “-100” 填充\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # 如果token在word_ids内，且word_idx不为None，则从labels_to_ids获取label id\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        # 和上一个属于同一个词，则则从labels_to_ids获取label id\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "# 构建自己的数据集类\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        # tokenizer 向量化文本\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i), \n",
    "                                padding='max_length', \n",
    "                                max_length = 512, \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\") for i in txt]\n",
    "        # 对齐标签\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        return text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404a0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(100, 2)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dbdf46",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a71b9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertNer, self).__init__()\n",
    "        \n",
    "        # BertForTokenClassification模型是针对序列标注任务进行优化，默认损失函数是交叉熵损失函数\n",
    "        # num_labels：分类任务的类别数量\n",
    "        self.bert = BertForTokenClassification.from_pretrained(model_path,\n",
    "                                                               num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, \n",
    "                           attention_mask=mask,\n",
    "                           labels=label, \n",
    "                           return_dict=False)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fd328",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d55d798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../../../models/bert-base-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 1\n",
    "model = BertNer()\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "#!pip install tdqm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbc5d491-33b2-40b7-9334-ae817414b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "----------------------------------------\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSequence(df_train)\n",
    "val_dataset = DataSequence(df_val)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].keys())\n",
    "print('-' * 40)\n",
    "print(train_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa5bd3a0-cfbc-4b48-8b4a-94c164e7e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义训练和验证集数据\n",
    "train_dataset = DataSequence(df_train)\n",
    "val_dataset = DataSequence(df_val)\n",
    "# 批量获取训练和验证集数据\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=1, batch_size=8)\n",
    "# 判断是否使用GPU，如果有，尽量使用，可以加快训练速度\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# 定义优化器\n",
    "# optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "optim = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                       num_warmup_steps=0,\n",
    "                                       num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969fc1f-3bf9-484e-b725-c187d6d7122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "for x, y in train_dataloader:\n",
    "    train_label = train_label[0]\n",
    "    mask = train_data['attention_mask'][0]\n",
    "    input_id = train_data['input_ids'][0]\n",
    "\n",
    "    print(train_label.shape)\n",
    "    print(mask.shape)\n",
    "    print(input_id.shape)\n",
    "\n",
    "    a=asas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260c3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, df_train, df_val):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    # 开始训练循环\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        # 训练模型\n",
    "        model.train()\n",
    "        # 按批量循环训练模型\n",
    "        for train_data, train_label in train_dataloader:\n",
    "              # 从train_data中获取mask和input_id\n",
    "            train_label = train_label[0].to(device)\n",
    "            mask = train_data['attention_mask'][0].to(device)\n",
    "            input_id = train_data['input_ids'][0].to(device)\n",
    "            \n",
    "            # 梯度清零！！\n",
    "            optim.zero_grad()\n",
    "            # 输入模型训练结果：损失及分类概率\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "            # 过滤掉特殊token及padding的token\n",
    "            logits_clean = logits[0][train_label != -100]\n",
    "            label_clean = train_label[train_label != -100]\n",
    "            # 获取最大概率值\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "      # 计算准确率\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "      # 反向传递\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "        # 模型评估\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        for val_data, val_label in val_dataloader:\n",
    "      # 批量获取验证数据\n",
    "            val_label = val_label[0].to(device)\n",
    "            mask = val_data['attention_mask'][0].to(device)\n",
    "            input_id = val_data['input_ids'][0].to(device)\n",
    "      # 输出模型预测结果\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "      # 清楚无效token对应的结果\n",
    "            logits_clean = logits[0][val_label != -100]\n",
    "            label_clean = val_label[val_label != -100]\n",
    "            # 获取概率值最大的预测\n",
    "            predictions = logits_clean.argmax(dim=1)          \n",
    "            # 计算精度\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'''Epochs: {epoch_num + 1} | \n",
    "                Loss: {total_loss_train / len(df_train): .3f} | \n",
    "                Accuracy: {total_acc_train / len(df_train): .3f} |\n",
    "                Val_Loss: {total_loss_val / len(df_val): .3f} | \n",
    "                Accuracy: {total_acc_val / len(df_val): .3f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88caec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, df_train, df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae5b50-ae5a-4843-aedf-9b66fc335edd",
   "metadata": {},
   "source": [
    "> 注意，有一个重要的步骤！在训练循环的每个 epoch 中，在模型预测之后，需要忽略所有以 \"-100\" 作为标签的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36aae6-dbd1-46af-b9a6-d7cb1762836a",
   "metadata": {},
   "source": [
    "# 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ed65b-5d30-42dc-af54-c344ee875379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, df_test):\n",
    "    # 定义测试数据\n",
    "    test_dataset = DataSequence(df_test)\n",
    "    # 批量获取测试数据\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "   # 使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    total_acc_test = 0.0\n",
    "    for test_data, test_label in test_dataloader:\n",
    "        test_label = test_label[0].to(device)\n",
    "        mask = test_data['attention_mask'][0].to(device)\n",
    "        input_id = test_data['input_ids'][0].to(device)\n",
    "          \n",
    "        loss, logits = model(input_id, mask, test_label.long())\n",
    "        logits_clean = logits[0][test_label != -100]\n",
    "        label_clean = test_label[test_label != -100]\n",
    "        predictions = logits_clean.argmax(dim=1)             \n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_test += acc\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ca614-aebb-4a25-ac3f-8b0280db0911",
   "metadata": {},
   "source": [
    "# 使用经过训练的模型来预测文本或句子中每个单词的实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfcfbc-551a-413e-8b99-f3228eec2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(texts): \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                # NOTE：这里的1并不是真是label，只要不是-100即可\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    return label_ids\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "    \n",
    "    logits = model(input_id, mask, None)\n",
    "    print(logits)\n",
    "    # NOTE: 预测时没有label，所以模型只会返回logits，没有loss\n",
    "    # 去除-100标记\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "    \n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "            \n",
    "evaluate_one_text(model, '侯海伦在石家庄的御芝林公司担任算法工程师')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfce7d-28ec-48ca-9b7c-f287ac5b265c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
